{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07c1e3b9",
      "metadata": {
        "id": "07c1e3b9"
      },
      "source": [
        "# Internal Knowledge Base Q&A Using Langchain & OpenAI\n",
        "\n",
        "This example shows how to query an internal knowledge base stored in a GitHub repo as Markdown files.\n",
        "\n",
        "This notebook is adapted from the [Retrieval Question Answering with Sources](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html) example by Langchain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up\n"
      ],
      "metadata": {
        "id": "0UqlAAxTXnGF"
      },
      "id": "0UqlAAxTXnGF"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.123 # https://github.com/hwchase17/langchain/releases\n",
        "!pip install openai\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "NANhNz315A6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b031c4-7ee0-434c-8b63-28a74077b369"
      },
      "id": "NANhNz315A6w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain==0.0.123\n",
            "  Downloading langchain-0.0.123-py3-none-any.whl (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.123) (6.0)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.123) (1.4.47)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.123) (8.2.2)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.123) (1.10.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.123) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.123) (2.27.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.123) (22.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.123) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain==0.0.123) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.123) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.123) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain==0.0.123) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.123) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.123) (23.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.123 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 typing-inspect-0.8.0 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up OPEN_API_KEY and necessary variables"
      ],
      "metadata": {
        "id": "TUbD-eXCXNy-"
      },
      "id": "TUbD-eXCXNy-"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key here and hit enter:\")"
      ],
      "metadata": {
        "id": "tY7CJZoh5cma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491447db-efac-43b1-ac80-efa1baac68b4"
      },
      "id": "tY7CJZoh5cma",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI API key here and hit enter:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REPO_URL = \"https://github.com/GovTechSG/developer.gov.sg\"  # Source URL\n",
        "DOCS_FOLDER = \"docs\"  # Folder to check out to\n",
        "REPO_DOCUMENTS_PATH = \"collections/_products/categories/devops/ship-hats\"  # Set to \"\" to index the whole data folder\n",
        "DOCUMENT_BASE_URL = \"https://www.developer.tech.gov.sg/products/categories/devops/ship-hats\"  # Actual URL\n",
        "DATA_STORE_DIR = \"data_store\""
      ],
      "metadata": {
        "id": "ze4wgstKYprq"
      },
      "id": "ze4wgstKYprq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the datastore\n",
        "*(Skip to next section to load data store from files if it has been saved locally to save cost of embeddings)*"
      ],
      "metadata": {
        "id": "1mHZRgDKXv1r"
      },
      "id": "1mHZRgDKXv1r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the GitHub repo"
      ],
      "metadata": {
        "id": "Kwk0Vu46YWjQ"
      },
      "id": "Kwk0Vu46YWjQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone $REPO_URL $DOCS_FOLDER"
      ],
      "metadata": {
        "id": "vIPGgokKYNf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0f2529-006a-4409-88bd-7c338c5b5e7e"
      },
      "id": "vIPGgokKYNf-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'docs'...\n",
            "remote: Enumerating objects: 38440, done.\u001b[K\n",
            "remote: Counting objects: 100% (1657/1657), done.\u001b[K\n",
            "remote: Compressing objects: 100% (993/993), done.\u001b[K\n",
            "remote: Total 38440 (delta 1235), reused 972 (delta 648), pack-reused 36783\u001b[K\n",
            "Receiving objects: 100% (38440/38440), 465.55 MiB | 30.81 MiB/s, done.\n",
            "Resolving deltas: 100% (25703/25703), done.\n",
            "Updating files: 100% (1801/1801), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load documents and split them into chunks for conversion to embeddings"
      ],
      "metadata": {
        "id": "tfkgVeTzNv1x"
      },
      "id": "tfkgVeTzNv1x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c7049db",
      "metadata": {
        "id": "5c7049db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8dd5b2-e651-4c39-bc30-9bdc9048a5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Created a chunk of size 1597, which is longer than the specified 1000\n",
            "WARNING:root:Created a chunk of size 1295, which is longer than the specified 1000\n",
            "WARNING:root:Created a chunk of size 2141, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "name_filter = \"**/*.md\"\n",
        "separator = \"\\n### \"  # This separator assumes Markdown docs from the repo uses ### as logical main header most of the time\n",
        "chunk_size_limit = 1000\n",
        "max_chunk_overlap = 20\n",
        "\n",
        "repo_path = pathlib.Path(os.path.join(DOCS_FOLDER, REPO_DOCUMENTS_PATH))\n",
        "document_files = list(repo_path.glob(name_filter))\n",
        "\n",
        "def convert_path_to_doc_url(doc_path):\n",
        "  # Convert from relative path to actual document url\n",
        "  return re.sub(f\"{DOCS_FOLDER}/{REPO_DOCUMENTS_PATH}/(.*)\\.[\\w\\d]+\", f\"{DOCUMENT_BASE_URL}/\\\\1\", str(doc_path))\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=open(file, \"r\").read(),\n",
        "        metadata={\"source\": convert_path_to_doc_url(file)}\n",
        "    )\n",
        "    for file in document_files\n",
        "]\n",
        "\n",
        "text_splitter = CharacterTextSplitter(separator=separator, chunk_size=chunk_size_limit, chunk_overlap=max_chunk_overlap)\n",
        "split_docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional) Check estimated tokens and costs"
      ],
      "metadata": {
        "id": "M_9VndAJOJJa"
      },
      "id": "M_9VndAJOJJa"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "zW5htfir4fY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a9f368-a6be-47de-fd41-f1f1164efd34"
      },
      "id": "zW5htfir4fY0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "# create a GPT-4 encoder instance\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "total_word_count = sum(len(doc.page_content.split()) for doc in split_docs)\n",
        "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in split_docs)\n",
        "\n",
        "print(f\"\\nTotal word count: {total_word_count}\")\n",
        "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
        "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.0004 / 1000}\")"
      ],
      "metadata": {
        "id": "DTBz6iMdOGSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe6e4af-8a89-46b6-d4d8-9ff998bb2a06"
      },
      "id": "DTBz6iMdOGSG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total word count: 2065\n",
            "\n",
            "Estimated tokens: 5215\n",
            "\n",
            "Estimated cost of embedding: $0.002086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Vector Store using OpenAI"
      ],
      "metadata": {
        "id": "QpTi97ZFON4b"
      },
      "id": "QpTi97ZFON4b"
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vector_store = FAISS.from_documents(split_docs, embeddings)"
      ],
      "metadata": {
        "id": "vPpoCQTdDiSX"
      },
      "id": "vPpoCQTdDiSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify content of Vector Store with a sample query"
      ],
      "metadata": {
        "id": "4Qs1BnHCOWLp"
      },
      "id": "4Qs1BnHCOWLp"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "search_result = vector_store.similarity_search_with_score(\"What is SHIP-HATS?\")\n",
        "search_result\n",
        "\n",
        "line_separator = \"\\n\"# {line_separator}Source: {r[0].metadata['source']}{line_separator}Score:{r[1]}{line_separator}\n",
        "display(Markdown(f\"\"\"\n",
        "## Search results:{line_separator}\n",
        "{line_separator.join([\n",
        "  f'''\n",
        "  ### Source:{line_separator}{r[0].metadata['source']}{line_separator}\n",
        "  #### Score:{line_separator}{r[1]}{line_separator}\n",
        "  #### Content:{line_separator}{r[0].page_content}{line_separator}\n",
        "  '''\n",
        "  for r in search_result\n",
        "])}\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "wSrEU16R20AE"
      },
      "id": "wSrEU16R20AE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Save vector store to files and download/save in another location for reuse"
      ],
      "metadata": {
        "id": "VOdewvujOuaP"
      },
      "id": "VOdewvujOuaP"
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.save_local(DATA_STORE_DIR)\n",
        "# Download the files `$DATA_STORE_DIR/index.faiss` and `$DATA_STORE_DIR/index.pkl` to local"
      ],
      "metadata": {
        "id": "PaSz8jG3jF0Q"
      },
      "id": "PaSz8jG3jF0Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To load the Vector Store from files:"
      ],
      "metadata": {
        "id": "aZBxnIlnPHLs"
      },
      "id": "aZBxnIlnPHLs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the files `$DATA_STORE_DIR/index.faiss` and `$DATA_STORE_DIR/index.pkl` to local\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "if os.path.exists(DATA_STORE_DIR):\n",
        "  vector_store = FAISS.load_local(\n",
        "      DATA_STORE_DIR,\n",
        "      OpenAIEmbeddings()\n",
        "  )\n",
        "else:\n",
        "  print(f\"Missing files. Upload index.faiss and index.pkl files to {DATA_STORE_DIR} directory first\")"
      ],
      "metadata": {
        "id": "-sDZgGPMmTjS"
      },
      "id": "-sDZgGPMmTjS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "35f99145",
      "metadata": {
        "id": "35f99145"
      },
      "source": [
        "## Query using the vector store with ChatGPT integration\n",
        "### Set up the chat model and specific prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a49412",
      "metadata": {
        "id": "32a49412"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
        "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
        "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
        "----------------\n",
        "{summaries}\"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "]\n",
        "prompt = ChatPromptTemplate.from_messages(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3018f865",
      "metadata": {
        "id": "3018f865"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)  # Modify model_name if you have access to GPT-4\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "def print_result(result):\n",
        "  output_text = f\"\"\"### Question:\n",
        "  {query}\n",
        "  ### Answer:\n",
        "  {result['answer']}\n",
        "  ### Sources:\n",
        "  {result['sources']}\n",
        "  ### All relevant sources:\n",
        "  {' '.join(list(set([doc.metadata['source'] for doc in result['source_documents']])))}\n",
        "  \"\"\"\n",
        "  display(Markdown(output_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use the chain to query"
      ],
      "metadata": {
        "id": "eNjGnZME_DNU"
      },
      "id": "eNjGnZME_DNU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032a47f8",
      "metadata": {
        "id": "032a47f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "b01886fd-9282-4d2c-e6d2-de81addfa6ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question: \n  What is SHIP-HATS?\n  ### Answer: \n  **SHIP (Secure Hybrid Integration Pipeline)-HATS (Hive Agile Testing Solutions)** is a Continuous Integration/Continuous Delivery (CI/CD) component within SG Government Tech Stack (SGTS) that enables developers to plan, build, test, and deploy code to production. It is a multi-tenanted Software-as-a-Service (SaaS) based end-to-end CI/CD for all applications that is classified as RESTRICTED and below. It comes with security and governance guardrails to ensure policy compliance, better quality, visibility, and transparency. SHIP-HATS also offers shortened time-to-market, economies of scale, and a performance management dashboard. It is managed by GovTech, and it offers commercially off-the-shelf (COTS) tools with the right security and compliance settings. \n\n\n  ### Sources: \n  https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/overview\n  ### All relevant sources:\n  https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/resources https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/overview https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/training/tools\n  "
          },
          "metadata": {}
        }
      ],
      "source": [
        "query = \"What is SHIP-HATS?\"\n",
        "result = chain(query)\n",
        "print_result(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn on debugging to see the OpenAI requests"
      ],
      "metadata": {
        "id": "uDik18TWElmF"
      },
      "id": "uDik18TWElmF"
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.getLogger(\"openai\").setLevel(logging.DEBUG) # logging.INFO or logging.DEBUG\n",
        "\n",
        "query = \"What is SHIP-HATS?\"\n",
        "result = chain(query)\n",
        "print_result(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "JVQPEsF8Ej9U",
        "outputId": "d600a4a3-6d7d-4931-904d-855b5a5ebd7e"
      },
      "id": "JVQPEsF8Ej9U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/engines/text-embedding-ada-002/embeddings\n",
            "DEBUG:openai:api_version=None data='{\"input\": [\"What is SHIP-HATS?\"], \"encoding_format\": \"base64\"}' message='Post details'\n",
            "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/engines/text-embedding-ada-002/embeddings processing_ms=16 request_id=d9117df6d84956864393935607df48e8 response_code=200\n",
            "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
            "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"Use the following pieces of context to answer the users question.\\\\nTake note of the sources and include them in the answer in the format: \\\\\"SOURCES: source1 source2\\\\\", use \\\\\"SOURCES\\\\\" in capital letters regardless of the number of sources.\\\\nIf you don\\'t know the answer, just say that \\\\\"I don\\'t know\\\\\", don\\'t try to make up an answer.\\\\n----------------\\\\nContent: What is SHIP-HATS?\\\\n\\\\n**SHIP (Secure Hybrid Integration Pipeline)-HATS (Hive Agile Testing Solutions)** is Continuous Integration/Continuous Delivery [CI/CD](https://en.wikipedia.org/wiki/CI/CD){:target=\\\\\"_blank\\\\\"} component within SG Government Tech Stack (SGTS) with security and governance guardrails that enables developers to plan, build, test and deploy code to production.\\\\n\\\\nThis is a multi-tenanted Software-as-a-Service (SaaS) based end-to-end CI/CD for all applications that is classified as RESTRICTED and below.\\\\n\\\\n[Why is CI/CD important?](https://youtu.be/RlZCyexsJBc?t=260){:Target=\\\\\"_blank\\\\\"}\\\\nSource: https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/overview\\\\n\\\\nContent: Benefits of SHIP-HATS\\\\n\\\\n1. **Policy Compliance:** Standardised development tools and environment that is set up in compliance with **Application Infrastructure Architecture Standard (AIAS)** and **Instruction Manual 8 on Information Technology (IM8)** policies.\\\\n2. **Better Quality:** Automated security testing detects vulnerabilities early in the development cycle helping to deliver high quality applications. SHIP-HATS allows agencies to follow prescriptive workflow templates and share knowledge among themselves.\\\\n3. **Visibility & Transparency:** Agencies will always have access to source code and hence a better understanding of how the project is progressing.\\\\n4. **Shortened Time-to-market:** Agencies can shorten the time to market by leveraging on SHIP-HATS CI/CD tools with predefined & re-usable configurations into their system and do not have to invest in resources/time to do procurement with different commercial providers. Agencies also do not have to invest resources to re-train staff who were re-deployed across agencies.\\\\n5. **Economies of Scale:** SHIP-HATS purchases licenses in bulk and redistributes them for use in smaller quantities to agencies and is offered as a complete package with no hidden cost.\\\\n6. **GovTech Managed:** The CI/CD tools are procured and managed by GovTech letting you focus on your core application.\\\\n7. **Performance Management Dashboard:** Value-Stream Measurement capabilities that allow Agencies to capture key industry metrics, such as lead time to deployment and deployment frequency, to monitor the effectiveness of their DevSecOps practices\\\\nSource: https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/overview\\\\n\\\\nContent: SHIP-HATS \\\\n\\\\n| Developers |  Project Managers, Business Analysts  |\\\\n| :------------- | ----------------------------------------------------------------------------------------- |\\\\n| [SHIP-HATS Architectural Approach](https://www.youtube.com/watch?v=yiD4--KSdTI){:target=\\\\\"_blank\\\\\"}<br />[Roadmap](./overview#roadmap)<br /><br />[User Roles & Permissions](https://docs.developer.gov.sg/docs/ship-hats-documentation/#/user-roles-permissions){:target=\\\\\"_blank\\\\\"}<br /> [Security Testing 101](https://www.youtube.com/watch?v=SVomPCqKGM4){:target=\\\\\"_blank\\\\\"}<br />[Continuous Delivery](https://www.youtube.com/watch?v=DMMhqLKHLx0){:target=\\\\\"_blank\\\\\"} | [SHIP-HATS Overview for Newbies](https://www.youtube.com/watch?v=SVomPCqKGM4){:target=\\\\\"_blank\\\\\"}<br />[Understanding Subscription](./subscriptions){:target=\\\\\"_blank\\\\\"}<br /><br />[Request trial (for Public Officers)](./subscription#11-can-i-request-for-a-trial-subscription){:target=\\\\\"_blank\\\\\"}\\\\nSource: https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/resources\\\\n\\\\nContent: ---\\\\ntitle: Tools in SHIP-HATS\\\\nlayout: layout-page-sidenav\\\\ndescription: insert description.\\\\npublished: false\\\\n---\\\\n\\\\n### Overview\\\\n \\\\n**Commercially Off the Shelf (COTS)** tools are available on SHIP-HATS with right security and compliance settings. Here are curated links to documentation and tutorials to first learn the tools offered under **SHIP-HATS**. Note this is not specific to SHIP-HATS but a pre-cursor so you can use these tools within SHIP-HATS effectively.\\\\nSource: https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/training/tools\"}, {\"role\": \"user\", \"content\": \"What is SHIP-HATS?\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 256, \"stream\": false, \"n\": 1, \"temperature\": 0}' message='Post details'\n",
            "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=6379 request_id=bd38d7f62bdab86998ffb60095c4b121 response_code=200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question: \n  What is SHIP-HATS?\n  ### Answer: \n  **SHIP (Secure Hybrid Integration Pipeline)-HATS (Hive Agile Testing Solutions)** is a Continuous Integration/Continuous Delivery (CI/CD) component within SG Government Tech Stack (SGTS) that enables developers to plan, build, test, and deploy code to production. It is a multi-tenanted Software-as-a-Service (SaaS) based end-to-end CI/CD for all applications that is classified as RESTRICTED and below. It comes with security and governance guardrails to ensure policy compliance, better quality, visibility, and transparency. SHIP-HATS also offers shortened time-to-market, economies of scale, and a performance management dashboard. It is managed by GovTech, and it offers commercially off-the-shelf (COTS) tools with the right security and compliance settings. \n\n\n  ### Sources: \n  https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/overview\n  ### All relevant sources:\n  https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/resources https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/overview https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/training/tools\n  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print result again without rerunning"
      ],
      "metadata": {
        "id": "avYBDag5-5Zo"
      },
      "id": "avYBDag5-5Zo"
    },
    {
      "cell_type": "code",
      "source": [
        "print_result(result)"
      ],
      "metadata": {
        "id": "1LXrhU6h9OSe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "4257221c-f194-4d2b-8441-dd410d157e5c"
      },
      "id": "1LXrhU6h9OSe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Question: \n  What is SHIP-HATS?\n  ### Answer: \n  **SHIP (Secure Hybrid Integration Pipeline)-HATS (Hive Agile Testing Solutions)** is a Continuous Integration/Continuous Delivery (CI/CD) component within SG Government Tech Stack (SGTS) that enables developers to plan, build, test, and deploy code to production. It is a multi-tenanted Software-as-a-Service (SaaS) based end-to-end CI/CD for all applications that is classified as RESTRICTED and below. It comes with security and governance guardrails to ensure policy compliance, better quality, visibility, and transparency. SHIP-HATS also offers shortened time-to-market, economies of scale, and a performance management dashboard. It is managed by GovTech, and it offers commercially off-the-shelf (COTS) tools with the right security and compliance settings. \n\n\n  ### Sources: \n  https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/overview\n  ### All relevant sources:\n  https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/training/tools https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/overview https://www.developer.tech.gov.sg/products/categories/devops/ship-hats/resources\n  "
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}